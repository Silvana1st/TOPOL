{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Topol/digital fellowships_pca.png\" width=\"100%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:right;\">\n",
    "    <tr>\n",
    "        <td>                      \n",
    "            <div style=\"text-align: right\"><a href=\"https://alandavies.netlify.com\" target=\"_blank\">Dr Alan Davies</a></div>\n",
    "            <div style=\"text-align: right\">Lecturer health data science</div>\n",
    "            <div style=\"text-align: right\">University of Manchester</div>\n",
    "         </td>\n",
    "         <td>\n",
    "             <img src=\"https://github.com/i3hsInnovation/resources/blob/efa61022d0b8893200dad308f6590e694291f8c7/images/alan.PNG?raw=true\" width=\"30%\" />\n",
    "         </td>\n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis (PCA)\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About this Notebook\n",
    "This notebook introduces the concept of Principal Component Analysis (PCA) for Machine Learning and is at <code>Beginner</code> level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Learning Objectives:</b> \n",
    "<br/> At the end of this notebook you will be able to:\n",
    "    \n",
    "- Investigate key features of Principle Components Analysis\n",
    "\n",
    "- Explore some essential Machine Learning Python libraries to implement Machine Learning algorithms\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "<b>Table of contents</b><br/>\n",
    "\n",
    "1.0 [About PCA](#aboutpca)\n",
    "\n",
    "2.0 [PCA For Visualisation](#pcavisualisation)\n",
    "\n",
    "3.0 [PCA for Feature Selection and Dimensionality Reduction](#featureselect)\n",
    "\n",
    "4.0 [Your Turn](#yourturn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"aboutpca\"></a>\n",
    "\n",
    "## About PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is one of the most popular <code>unsupervised</code> methods for <code>dimensionality reduction</code>. PCA can also be used for other things too, such as: \n",
    "<ul>\n",
    "    <li>Data compression</li>\n",
    "    <li>Visualization of higher dimensional data</li>\n",
    "    <li>Speeding up Machine Learning algorithms</li>\n",
    "    <li>Feature extraction</li>\n",
    "    <li>Removing (denoising) noise from data</li>\n",
    "</ul>\n",
    "PCA is very useful when there is a lot of <code>multicolinearity</code> between features or when you have a lot of features (high dimensional data). Examples of high dimensional data can include genomic data and health data, for example there can be over 100 features like (immune status, blood pressure, heart rate, previously diagnosed conditions, blood biomarkers, ...) in some health datasets. \n",
    "Unlike Linear Regression (see notebook on <a href=\"https://www.bbc.co.uk/news\" target=\"_blank\">Linear Regression link</a> for more details) where we wanted to predict one variable based on another, PCA is concerned with the relationship between the data values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> Multicollinearity is where $\\geq$ 2 explanatory variables are highly linearly related in a regression model. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong>\n",
    "    Essentially PCA finds the <code>hyperplane</code> closest to the data in order to project the data onto it.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is determine which plane preserves the most variance. In the example below we can see that most of the variance in this dataset is along the red line in <code>A</code>. PCA finds the axis that represents the largest amount of variance in the data. It then finds the next largest axis and so on. It then creates these axes orthoganally (at right angles) to the previous axes (<code>B</code>). In this case there are just 2. The <code>principle component</code> is the vector that defines the $i^{th}$ axis. Put another way, these components are a series of linear transformations that aligns the feature that accounts for the greatest variance on the first axis, the next highest variance on the next axis and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>Note:</strong>\n",
    "    The number of dimensions will be the same as the number of features. \n",
    "</div>\n",
    "\n",
    "<b>any reason why this note is red?  Does it need explaining a bit more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Topol/pca1.png\" width=\"100%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong>\n",
    "A vector is a quantity with both a magnitude and direction. It is useful to be familiar with the these concepts for Machine Learning in general:<br>\n",
    "    \\[ \n",
    "1, \n",
    "\\begin{bmatrix} 1 \\\\[0.3em] 2 \\end{bmatrix},  \n",
    "\\begin{bmatrix} 1 & 2 \\\\[0.3em] 3 & 4 \\end{bmatrix},  \n",
    "\\begin{bmatrix} \\begin{bmatrix} 1 & 2 \\end{bmatrix} & \\begin{bmatrix} 3 & 4 \\end{bmatrix} \\\\[0.3em] \\begin{bmatrix} 5 & 6 \\end{bmatrix} & \\begin{bmatrix} 7 & 8 \\end{bmatrix} \\\\[0.3em] \n",
    "\\end{bmatrix}  \n",
    "\\]\n",
    "From left to right separated by commas: A <strong>scalar</strong> (single value), a <strong>vector</strong>, a <strong>matrix</strong> and a <strong>tensor</strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then rotate the data. Here we see that most of this data can be represented in a single dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Topol/pca2.png\" width=\"40%\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an example below produced with the <code>mglearn</code> package. The top left image shows the original data with the 2 principle components. The top right shows what the data looks like when it has been transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Topol/mglearn.png\" width=\"60%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>Note:</strong>\n",
    "    When we transform data from higher dimensional space to lower dimensional (i.e. 4D to 2D) space we loose some information (variance).\n",
    "</div>\n",
    "\n",
    "<b>Would an additonal 4D image help here to illustrate where the image loses variance </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to we create the principle components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much difference between <code>variance</code> (see <b>notebook on linear regression</b> for more details on variance) and <code>covariance</code>. Covariance measures the joint variability of two variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\\text{var}(x) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{(n-1)}$$\n",
    "<br>\n",
    "$$\\text{cov}(x,y) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{(n-1)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use covariance to create a <code>covariance matrix</code>. Computing the covariance matrix is the next step following standardization of the data (discussed later). The matrix shows how co-dependent two variables are together:<br>\n",
    "$$\\begin{bmatrix} \\text{var(x)} & \\text{cov(x,y)} \\\\[0.3em] \\text{cov(y,x)} & \\text{var(y)} \\end{bmatrix}$$\n",
    "$\\text{cov(x,y)}$ is equal to $\\text{cov(y,x)}$. The covariance matrix defines both the variance and covariance of the data, or in other words the spread and orientation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors and eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first (longer) red line in the image above was the first eigenvector, the second (shorter) red line is the second eigenvector. Eigenvectors have a value called an eigenvalue. The direction is the eigenvector, the magnitude is the eigenvalue. Eigenvectors of a covariance matrix are orthogonal (at right angles) to one another. They are used to move (transform) the x/y axes to those represented by the principle components.\n",
    "<b>Maybe reference the image above so its easier to find?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately Python's <code>sklearn</code> library contains a PCA class that performs these stages for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to top](#top)\n",
    "\n",
    "\n",
    "----------\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"pcavisualisation\"></a>\n",
    "\n",
    "## PCA for Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at an example using the Breast cancer dataset and use <code>sklearn</code> library which has some datasets that are built-in. \n",
    "<br/>\n",
    "First let's import the Breast cancer dataset from the <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\">UCI ML Breast Cancer Wisconsin (Diagnostic)</a> and then import <code>pandas</code> module as it it useful for data science and supports object like <code>data frames</code>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 1:</b>\n",
    "<br> \n",
    "Have a look at the link to the data presented in the link above. <br>\n",
    "    1. What information is presented about the data?<br>\n",
    "    2. What information does the dataset capture?\n",
    "</div>\n",
    "\n",
    "Now you've noted some of the answers to the above questions down, now let's load the dataset and import the modules needed to visualise the data.  This is shown in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we have loaded the dataset into a variable called <code>cancer</code>. The other lines of code make sure that we have the correct column names. Finally on the last line, we store the data in a dataframe object called <code>df</code> (short for data frame) to make it easier to manipulate in a tabular form. Finally we will use the <code>head()</code> function to view the first 10 records of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.1556</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "5        12.45         15.70           82.57      477.1          0.12780   \n",
       "6        18.25         19.98          119.60     1040.0          0.09463   \n",
       "7        13.71         20.83           90.20      577.9          0.11890   \n",
       "8        13.00         21.82           87.50      519.8          0.12730   \n",
       "9        12.46         24.04           83.97      475.9          0.11860   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760         0.30010              0.14710         0.2419   \n",
       "1           0.07864         0.08690              0.07017         0.1812   \n",
       "2           0.15990         0.19740              0.12790         0.2069   \n",
       "3           0.28390         0.24140              0.10520         0.2597   \n",
       "4           0.13280         0.19800              0.10430         0.1809   \n",
       "5           0.17000         0.15780              0.08089         0.2087   \n",
       "6           0.10900         0.11270              0.07400         0.1794   \n",
       "7           0.16450         0.09366              0.05985         0.2196   \n",
       "8           0.19320         0.18590              0.09353         0.2350   \n",
       "9           0.23960         0.22730              0.08543         0.2030   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "5                 0.07613  ...          23.75           103.40       741.6   \n",
       "6                 0.05742  ...          27.66           153.20      1606.0   \n",
       "7                 0.07451  ...          28.14           110.60       897.0   \n",
       "8                 0.07389  ...          30.73           106.20       739.3   \n",
       "9                 0.08243  ...          40.68            97.65       711.4   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "5            0.1791             0.5249           0.5355                0.1741   \n",
       "6            0.1442             0.2576           0.3784                0.1932   \n",
       "7            0.1654             0.3682           0.2678                0.1556   \n",
       "8            0.1703             0.5401           0.5390                0.2060   \n",
       "9            0.1853             1.0580           1.1050                0.2210   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890     0.0  \n",
       "1          0.2750                  0.08902     0.0  \n",
       "2          0.3613                  0.08758     0.0  \n",
       "3          0.6638                  0.17300     0.0  \n",
       "4          0.2364                  0.07678     0.0  \n",
       "5          0.3985                  0.12440     0.0  \n",
       "6          0.3063                  0.08368     0.0  \n",
       "7          0.3196                  0.11510     0.0  \n",
       "8          0.4378                  0.10720     0.0  \n",
       "9          0.4366                  0.20750     0.0  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "data = np.c_[cancer.data, cancer.target]\n",
    "columns = np.append(cancer.feature_names, [\"target\"])\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you scroll along to the end of the dataframe, you can see there are a lot of features in this dataset. Specifically we can count the features (excluding the dependent variable) using the <code>len()</code> function on the columns in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features:\", len(df.columns)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data frame into features <code>X</code> and labels <code>y</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>Note:</strong>\n",
    "    You should apply <code>feature scaling</code> to your data before applying a PCA as it is sensitive to scale. We can use <code>from sklearn.preprocessing import StandardScaler</code> to scale features. Higher magnitude features lead to higher variance in those features. Feature scaling should be applied to the training data only.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "scaled_df = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can output the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09706398, -2.07333501,  1.26993369, ...,  2.29607613,\n",
       "         2.75062224,  1.93701461],\n",
       "       [ 1.82982061, -0.35363241,  1.68595471, ...,  1.0870843 ,\n",
       "        -0.24388967,  0.28118999],\n",
       "       [ 1.57988811,  0.45618695,  1.56650313, ...,  1.95500035,\n",
       "         1.152255  ,  0.20139121],\n",
       "       ...,\n",
       "       [ 0.70228425,  2.0455738 ,  0.67267578, ...,  0.41406869,\n",
       "        -1.10454895, -0.31840916],\n",
       "       [ 1.83834103,  2.33645719,  1.98252415, ...,  2.28998549,\n",
       "         1.91908301,  2.21963528],\n",
       "       [-1.80840125,  1.22179204, -1.81438851, ..., -1.74506282,\n",
       "        -0.04813821, -0.75120669]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is a method of normalisation for features. A lot of machine learning algorithms use distances. Imagine having 2 features with different scales, the differences between them would be different and would affect such distance calculations. For example the difference between house prices in hundreds of thousands of pounds/dollars compared to age in years. Both variables have very different magnitudes. To overcome this the features should be normalised to prevent this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong>\n",
    "    There are many different scalers to choose from. The <code>standard scaler</code> used here assumes that the data in each feature is normally distributed. It works by scaling the features so they have a mean of 0 and a standard deviation of 1:\n",
    "    $$x_\\text{new} = \\frac{x-\\mu}{\\sigma}$$\n",
    "    Where: $\\mu$ is the mean of the training samples and $\\sigma$ is the standard deviation. If you want to learn more about this, have a look at the <a href=\"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html\" target=\"_blank\">Importance of Feature Scaling</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the PCA on the scaled data. To do this we import the <code>PCA</code> class from the <code>sklearn.decomposition</code> package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create an instance of the PCA class and pass in the number of principle components we want. As we want to be able to visualize this data easily, we will choose 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can fit the scaled data and transform it into the required principle components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_trans = pca.transform(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check it worked lets view the number of columns (features) in <code>X</code> and then the number of columns in the scaled data <code>scaled_df</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features X: 30\n",
      "Number of features scaled_df: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features X:\", X.shape[1])\n",
    "print(\"Number of features scaled_df:\", pca_trans.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong>\n",
    "    The <code>shape()</code> function produces a <code>tuple</code> with two elements. The first describes the number of rows, the second the number of columns. To access the second element we provide an index in brackets (in Python the indexing system starts at 0, so 1 is the second element).  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(pca_trans[:,0],pca_trans[:,1],c=y);\n",
    "plt.xlabel('First principal component');\n",
    "plt.ylabel('Second principal component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have split the data into two principle components. The colour represents the class (cancer (yellow)/no-cancer (purple))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>Note:</strong>\n",
    "    If we omit the <code>n_components=?</code> from the pca i.e. <code>pca = PCA()</code> then all the features will be included.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to top](#top)\n",
    "\n",
    "\n",
    "----------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"featureselect\"></a>\n",
    "\n",
    "## PCA for Feature Selection and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use PCA to determine the importance of various features in our dataset. This can help us to choose which features we should include and exclude from our models. The higher the magnitude, the more important the feature. To do this we can look at which features are the most important for each principle component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21890244 0.10372458 0.22753729 0.22099499 0.14258969 0.23928535\n",
      "  0.25840048 0.26085376 0.13816696 0.06436335 0.20597878 0.01742803\n",
      "  0.21132592 0.20286964 0.01453145 0.17039345 0.15358979 0.1834174\n",
      "  0.04249842 0.10256832 0.22799663 0.10446933 0.23663968 0.22487053\n",
      "  0.12795256 0.21009588 0.22876753 0.25088597 0.12290456 0.13178394]\n",
      " [0.23385713 0.05970609 0.21518136 0.23107671 0.18611302 0.15189161\n",
      "  0.06016536 0.0347675  0.19034877 0.36657547 0.10555215 0.08997968\n",
      "  0.08945723 0.15229263 0.20443045 0.2327159  0.19720728 0.13032156\n",
      "  0.183848   0.28009203 0.21986638 0.0454673  0.19987843 0.21935186\n",
      "  0.17230435 0.14359317 0.09796411 0.00825724 0.14188335 0.27533947]]\n"
     ]
    }
   ],
   "source": [
    "print(abs(pca.components_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image below makes the distinction clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Topol/pcs.png\" width=\"70%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features for each component can be seen inbetween the square brackets []. Each row (represented by the two colours in the image above) represents a principle component and each value a feature. So the numbers with the highest values in each row are the most important features. For example in principle component 1 the highest value is 0.26085376 which is feature number 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 2:</b>\n",
    "<br> \n",
    "What is the value of most important feature in the second principle component?\n",
    "</div>\n",
    "\n",
    "<b>Instructions on what we are going to do next and follow up on which ...</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36657547137804547\n"
     ]
    }
   ],
   "source": [
    "print(max(abs(pca.components_)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the <code>explained variance ratio</code> which shows us the variance explained by each of the principle components. First we load the <code>train_test_split()</code> function and then split the data in the usual way into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we scale the data to avoid problems with different variables being on different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create and the PCA. Note that we omitted the <code>n_components</code> parameter. This means we will include all the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we store the explained variance ratio in a variable called <code>explained_variance</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a dataframe to view the data as a table showing the component number $(1...n)$ and the explained variance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principle component</th>\n",
       "      <th>Explained variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.434308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.197401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.093518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.066777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.056425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.040716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.021493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.015127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.013968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.010122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.008972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.007714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.005553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.003115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.002646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.001996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.001706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.001061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.000995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.000547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Principle component  Explained variance\n",
       "0                     1            0.434308\n",
       "1                     2            0.197401\n",
       "2                     3            0.093518\n",
       "3                     4            0.066777\n",
       "4                     5            0.056425\n",
       "5                     6            0.040716\n",
       "6                     7            0.021493\n",
       "7                     8            0.015127\n",
       "8                     9            0.013968\n",
       "9                    10            0.011601\n",
       "10                   11            0.010122\n",
       "11                   12            0.008972\n",
       "12                   13            0.007714\n",
       "13                   14            0.005553\n",
       "14                   15            0.003115\n",
       "15                   16            0.002646\n",
       "16                   17            0.001996\n",
       "17                   18            0.001706\n",
       "18                   19            0.001504\n",
       "19                   20            0.001061\n",
       "20                   21            0.000995\n",
       "21                   22            0.000878\n",
       "22                   23            0.000724\n",
       "23                   24            0.000581\n",
       "24                   25            0.000547\n",
       "25                   26            0.000261\n",
       "26                   27            0.000216\n",
       "27                   28            0.000046\n",
       "28                   29            0.000025\n",
       "29                   30            0.000004"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Principle component': range(1,len(explained_variance)+1), 'Explained variance': abs(explained_variance)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 3:</b>\n",
    "<br> \n",
    "With regard to the table above what are the values of the top 3 principle components?<br>\n",
    "    If you want to do this with code, you can use the <code>nlargest()</code> function. More information about how to use it <a href=\"https://www.w3resource.com/pandas/dataframe/dataframe-nlargest.php\" target=\"_blank\">here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principle component</th>\n",
       "      <th>Explained variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.434308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.197401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.093518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Principle component  Explained variance\n",
       "0                    1            0.434308\n",
       "1                    2            0.197401\n",
       "2                    3            0.093518"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nlargest(3,['Explained variance','Principle component'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to work out how many components to include in our PCA is to plot the cumulative explained variance ratio against the number of components. So here we need around 20 components to describe 100% of the variance. 10 components describes around 95% of the variance. We can use this to determine how many components to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.grid()\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have removed the features with the least importance, you can evaluate the effectiveness by running a Machine Learning algorithm on the data. The chosen algorithm will depend on how you want to analyse the data and the qualities of the data. Running a PCA is often done after initial data processing and before trying to create a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong>\n",
    "    Data for the PCA is loaded into memory. With very large datasets one can run out of available memory. To overcome this there is an incremental PCA that uses batches of data (<code>from sklearn.decomposition import IncrementalPCA</code>).  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>Note:</strong>\n",
    "    PCA is prone to being affected by outliers in data. To overcome this there are more robust versions of PCA available. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to top](#top)\n",
    "\n",
    "\n",
    "----------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"yourturn\"></a>\n",
    "\n",
    "## Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try and apply what we have learned to a new dataset. This is the <code>pima-indians-diabetes</code> <a href=\"https://www.kaggle.com/kumargh/pimaindiansdiabetescsv/data\">dataset</a> from Kaggle that you saw previously. The data is from the medical records of people from India. Click on the dataset link above and read the description of the data fields (columns) in the section called <code>About this file</code> so that you understand what sort of data is contained in the dataset. Let's load the file and then display the first 10 records.\n",
    "\n",
    "<b>pima-indians-diabetes - says we've already looked at it - cant find where?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            1       85             66             29        0  26.6   \n",
       "1            8      183             64              0        0  23.3   \n",
       "2            1       89             66             23       94  28.1   \n",
       "3            0      137             40             35      168  43.1   \n",
       "4            5      116             74              0        0  25.6   \n",
       "5            3       78             50             32       88  31.0   \n",
       "6           10      115              0              0        0  35.3   \n",
       "7            2      197             70             45      543  30.5   \n",
       "8            8      125             96              0        0   0.0   \n",
       "9            4      110             92              0        0  37.6   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Class  \n",
       "0                     0.351   31      0  \n",
       "1                     0.672   32      1  \n",
       "2                     0.167   21      0  \n",
       "3                     2.288   33      1  \n",
       "4                     0.201   30      0  \n",
       "5                     0.248   26      1  \n",
       "6                     0.134   29      0  \n",
       "7                     0.158   53      1  \n",
       "8                     0.232   54      1  \n",
       "9                     0.191   30      0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='./Topol/pima-indians-diabetes.csv'\n",
    "data = pd.read_csv(path)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 4:</b>\n",
    "<br> Split the data into features <code>X</code> and labels <code>y</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:, 0:-1].values\n",
    "y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 5:</b>\n",
    "    <br> Import the <code>train_test_split</code> and split the data into <code>training</code> and <code>test</code> data for both features and labels.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 6:</b>\n",
    "    <br> Apply feature scaling using <code>StandardScaler()</code>, fit and transform the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 7:</b>\n",
    "    <br> Create an instance of PCA for all features and apply the <code>fit_transform()</code> and <code>transform()</code> functions to the <code>X_train</code> and <code>X_test</code> variables.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 8:</b>\n",
    "    <br> Output the components using <code>pca.components_</code> and the explained variance ration using <code>explained_variance_ratio_</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components:\n",
      "[[0.06984296 0.39112154 0.35684797 0.44535053 0.44802765 0.45932466\n",
      "  0.27926172 0.16362548]\n",
      " [0.5979397  0.17824165 0.21797755 0.30840148 0.24859608 0.02875326\n",
      "  0.10750265 0.62760558]\n",
      " [0.00697864 0.52707733 0.50302722 0.28558336 0.34156108 0.36210472\n",
      "  0.36668274 0.07284576]\n",
      " [0.03030215 0.34966335 0.04484526 0.00358517 0.32614326 0.0225011\n",
      "  0.8731363  0.07450267]\n",
      " [0.51649416 0.41555629 0.38356287 0.50552426 0.30588571 0.22268766\n",
      "  0.05192435 0.10963348]\n",
      " [0.15531002 0.12389244 0.60955381 0.02110757 0.29240521 0.70397541\n",
      "  0.04567141 0.07321217]\n",
      " [0.56049015 0.26152351 0.20809113 0.00276677 0.14718866 0.1208057\n",
      "  0.08489671 0.72847835]\n",
      " [0.17772597 0.39843189 0.09010873 0.60743128 0.56009503 0.31113146\n",
      "  0.04053856 0.14290053]]\n",
      "Explained variance ratio:\n",
      "[0.26221775 0.21796968 0.12816209 0.11005445 0.09516259 0.0858845\n",
      " 0.05325788 0.04729105]\n"
     ]
    }
   ],
   "source": [
    "print(\"Components:\")\n",
    "print(abs(pca.components_))\n",
    "print(\"Explained variance ratio:\")\n",
    "print(abs(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "### Notebook details\n",
    "<br>\n",
    "<i>Notebook created by <strong>Dr. Alan Davies</strong> with, <strong>Frances Hooley</strong> and <strong>Dr. Jon Parkinson</strong>\n",
    "\n",
    "Publish date: September 2020<br>\n",
    "Review date: Semptember 2021</i>\n",
    "\n",
    "Please give your feedback using the button below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"typeform-share button\" href=\"https://hub11.typeform.com/to/DssEuvB2\" data-mode=\"popup\" style=\"display:inline-block;text-decoration:none;background-color:#3A7685;color:white;cursor:pointer;font-family:Helvetica,Arial,sans-serif;font-size:18px;line-height:45px;text-align:center;margin:0;height:45px;padding:0px 30px;border-radius:22px;max-width:100%;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;font-weight:bold;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;\" target=\"_blank\">Rate this notebook </a> <script> (function() { var qs,js,q,s,d=document, gi=d.getElementById, ce=d.createElement, gt=d.getElementsByTagName, id=\"typef_orm_share\", b=\"https://embed.typeform.com/\"; if(!gi.call(d,id)){ js=ce.call(d,\"script\"); js.id=id; js.src=b+\"embed.js\"; q=gt.call(d,\"script\")[0]; q.parentNode.insertBefore(js,q) } })() </script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
